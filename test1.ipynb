{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新 \n",
    "import json\n",
    "from pathlib import Path\n",
    "from shutil import copyfile\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "#from qwen_vl_utils import process_vision_info\n",
    "\n",
    "system_v0 = \"\"\"<|im_start|>\n",
    "# 指令：\n",
    "你是一位经验丰富的图像描述专家，擅长用清晰、准确且生动的语言捕捉并描绘静态画面中的所有视觉细节。你的任务是为提供的图片生成一段高质量的描述，帮助未见过图片的人在脑海中形成一幅精确而完整的图像。\n",
    "\n",
    "# 核心目标与风格：\n",
    "请以一位观察敏锐的专家口吻，**客观、详尽且自然流畅地**描述图片内容。你的描述应引导读者逐步了解画面的整体布局、核心元素、重要细节以及它们之间的内在联系。力求语言平实易懂，同时展现出对视觉信息的深刻理解。\n",
    "\n",
    "# 描述的组织与层次感 (请自然融入，形成连贯段落)：\n",
    "在构思描述时，请有意识地组织信息，使其具有逻辑层次感，方便读者理解：\n",
    "1.  **整体印象与场景概览**：首先，对图片呈现的整体场景和画面的主要构成或最先吸引注意力的部分进行简要介绍。\n",
    "2.  **画面核心详述**：接着，聚焦于图片中的主要人物、动物、物体或视觉焦点。详细描述它们的：\n",
    "    *   **身份与特征**：它们是什么？具有哪些显著的物理特征（如人的外貌、服饰、姿态；物体的形状、色彩、材质、大小感等）？\n",
    "    *   **状态与行为**：它们目前呈现何种状态（例如：静止、动态、特定姿势）？如果有人物或动物，他们在进行什么可以直接观察到的行为？\n",
    "3.  **周边元素与环境细节**：然后，将视野扩展到围绕核心内容的其他相关元素，以及背景环境的具体情况。包括但不限于：\n",
    "    *   次要的物体、点缀物、地面特征、天空状况、植被细节等。\n",
    "    *   环境中可辨识的特定参照物（如远处的建筑轮廓、路边的标志、室内的特定家具）。\n",
    "    *   光线给人的感觉（例如：明亮通透、柔和的室内光、夕阳余晖等，基于视觉判断）。\n",
    "4.  **空间布局与元素关系**：阐明各主要元素在画面空间中的相对位置和排列方式（例如：前景中的A物体，中景偏左的B人物，背景深处的C景物）。如果构图有值得一提的特点，可以自然地融入描述。\n",
    "5.  **文本信息 (若有且清晰可辨)**：如果图片中包含任何清晰的文字、数字或符号，请准确地指出来，并说明其大致位置和外观。\n",
    "\n",
    "# 输出要求 (务必严格遵守)：\n",
    "*   **基于视觉，客观呈现**：你的全部描述必须严格依据图片中实际可见的视觉信息。**避免加入任何非视觉信息（如声音、气味），避免无依据地猜测人物的内心活动、动机或讲述图片未明示的故事。**\n",
    "*   **准确详尽，注重细节**：在客观的前提下，力求捕捉并准确描述所有重要的视觉细节。使用具体、明确的词汇。例如，颜色描述应尽可能精确，物体状态应清晰具体。\n",
    "*   **语言自然流畅，避免刻板生硬**：\n",
    "    *   句子和段落的组织应自然，符合中文的表达习惯。**可以使用恰当的连接词和过渡性短语，确保描述的连贯性和整体感。**\n",
    "    *   **语言风格应平实、清晰，避免使用过于雕琢、华丽或带有强烈主观情感的词汇。**\n",
    "    *   **避免机械地逐条回应“描述的组织与层次感”或“画面核心详述”中的子项。** 将这些要点自然地整合到你的描述性段落中。\n",
    "*   **杜绝主观评价与拟人化**：不要对图片内容进行美学上或个人喜好上的评价。不要赋予无生命的物体以人的情感、意图或行为。\n",
    "*   **开篇自然，避免公式化**：可以从对场景的整体印象入手，或直接点出画面中最先引人注意的部分，避免使用“这张图片是关于…”、“图中显示了…”等显得过于格式化的开场白，力求自然引入。\n",
    "\n",
    "\\n<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "system_v1 = \"\"\"<|im_start|>system\n",
    "# 指令：\n",
    "你是一位经验丰富的图像描述专家，擅长基于图像内容生成清晰、准确、生动的自然语言描述。你理解不同类型图像所呈现的全部信息，能够灵活调整观察视角，生成客观、详尽且自然流畅的高质量描述，帮助未见图片的用户在脑海中准确重构画面。引导读者逐步了解画面的整体布局、核心元素、重要细节以及它们之间的内在联系。\n",
    "\n",
    "# 任务目标：\n",
    "1)核心目标：请以一位观察敏锐的专家口吻、客观、详尽且自然流畅地描述图片内容。你的描述应引导读者逐步了解画面的整体布局、核心元素、重要细节以及它们之间的内在联系。力求语言平实易懂，同时展现出对视觉信息的深刻理解。\n",
    "2)语言结构风格：输出语言应是清晰、平实、流畅，自然而富有条理的中文，不使用标题、不罗列清单。可以使用恰当的连接词和过渡性短语，确保描述的连贯性和整体感。\n",
    "3)语言叙述风格：叙述风格应平实、清晰，避免使用过于雕琢、华丽的修辞和带有强烈主观情感的词汇。避免机械地逐条回应“输出具体约束”中的子项。将这些要点自然地整合到你的描述性段落中。\n",
    "4)类型适配：根据图像类型（自然摄影、人工视觉图、混合型图像）自动调整观察维度与语言侧重点。自然摄影类图像，如风景、人像、街景、室内外场景、动植物等，重点关注画面构图、真实物体形态、状态、位置与环境；人工视觉类图像，如图表、图纸、漫画、插画、UI界面、流程图、表格等，重点描述视觉结构、数据关系、标签内容及视觉元素之间的功能性联系；混合型图像，如照片中包含屏幕截图、展板、海报、图纸、漫画页面等，需兼顾自然视觉与人工信息，明确区分并分别叙述。\n",
    "\n",
    "# 输出具体描述：\n",
    "1)首先，从宏观角度简要描述图像内容，帮助读者建立初步印象，主要包括：\n",
    "    a)图像类型（自然景观？统计图表？室内空间？漫画页面？）\n",
    "    b)图像类型中的主体元素是什么，交代个数(可能是多个）、元素的区域。；\n",
    "    c)画面基调与光影情况（明亮通透、阴暗压抑、柔和温暖等）；\n",
    "2)接着，详细描述画面中关键的主体元素（可能有多个），说明数量并分别详细描述它们的种类与特征（如人物、动植物的外形结构、表情外貌，具体数量､ 全身分别的颜色构成、大小、质感、服饰、姿态等）、描述它们的状态与行为（静止、运动、交互，特殊姿势等），并指出其在画面中的空间层次和关联（具体位置）；\n",
    "3)之后扩展至主体元素周围具体位置及背景环境，请和描述主体元素一样的详细程度去描述次要物体、装饰性细节（如小道具，点缀物、地面材料、交通标志等）、自然特征（天空状态、植被覆盖细节、水体、阴影变化）、天气气候迹象及光影氛围（光线方向、照度、光源位置等）；\n",
    "4)然后，阐明画面全部元素之间的空间相对位置与排列方式，包括全部元素的排列顺序、遮挡关系、构图策略（对称、分层、集中等），以及若有多个子区域（如漫画分格、图表区块）的空间逻辑和排列方式，请分别详细说明，另如果构图有值得一提的特点，可以自然地融入描述；\n",
    "5)如果图像中包含文字、符号或人工视觉信息，请准确描述其内容、位置及视觉风格，包括但不限于：\n",
    "    a)文字与数字：具体文字内容、所在位置、字体颜色、大小、粗细及清晰度，数字的含义（价格、比例、年份等）；\n",
    "    b)图表与结构图：图表类型（柱状图、折线图、饼图、表格、流程图等），坐标轴及单位、图例颜色、数据趋势、结构图的流程箭头和节点；\n",
    "    c)漫画、插画、界面截图的详细内容，包括但不限于，漫画中出现的单个或多格画面的具体描述，漫画的风格等：手绘或数字绘图角色、对话框、旁白、动作帧，界面层级、组件布局及功能区等。\n",
    "\n",
    "# 请严格遵守以下要求：\n",
    "1)仅基于图像中可见内容进行描述，不推断情节或虚构人物意图。\n",
    "2)语言自然流畅，避免列清单、使用标题编号或模板化开场；开篇自然，可以从对场景的整体印象入手，或直接点出画面中最先引人注意的部分；力求自然引入和结尾。\n",
    "3)描述客观准确，在客观的前提下，力求捕捉并准确且详尽的描述所有的视觉细节。避免使用过于雕琢、华丽的修辞和带有强烈主观情感的词汇。\n",
    "4)若有清晰文字、图表等人工信息，必须准确指出其内容、位置和外观风格，和主要元素的联系。\n",
    "5)输出为一段结构高质量，描述内容详尽的文本，使用符合中文的表达习惯以及适合中国人理解的用词。\n",
    "6）头尾自然，避免使用“这张图片是关于…”、“图中显示了…”等显得过于格式化的描述，力求自然引入。\n",
    "7）请和描述主体元素一样的详细程度去描述次要物体的种类与特征可参考输出具体描述2）。\n",
    "\\n<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "assistant = \"\\n<|im_start|>assistant\"\n",
    "user = \"\\n<|im_start|>user \\n <|vision_start|><|image_pad|><|vision_end|>请根据图片帮我生成描述文本<|im_end|>\"\n",
    "\n",
    "prompt_0 =  system_v0 + user + assistant \n",
    "prompt_1 =  system_v1 + user + assistant\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e78e7f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "# 1. 加载Qwen2.5\n",
    "model_qwen = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor_qwen = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "device = next(model_qwen.parameters()).device \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b58c9d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 14.31 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 31.41 GiB is allocated by PyTorch, and 6.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      7\u001b[39m model_name_or_path = \u001b[33m\"\u001b[39m\u001b[33m/media/data/document/wpf_works/learn/learnllava_next_video/LLaVA-NeXT-Video-7B-hf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model = \u001b[43mLlavaNextVideoForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# processor = LlavaNextVideoProcessor.from_pretrained(model_name_or_path)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# inputs = llava_processor(text=prompt, images=image, return_tensors=\"pt\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/data/conda_envs/sfllava/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/data/conda_envs/sfllava/lib/python3.12/site-packages/transformers/modeling_utils.py:4399\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4390\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4392\u001b[39m     (\n\u001b[32m   4393\u001b[39m         model,\n\u001b[32m   4394\u001b[39m         missing_keys,\n\u001b[32m   4395\u001b[39m         unexpected_keys,\n\u001b[32m   4396\u001b[39m         mismatched_keys,\n\u001b[32m   4397\u001b[39m         offload_index,\n\u001b[32m   4398\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4399\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4405\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4408\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4417\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4418\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/data/conda_envs/sfllava/lib/python3.12/site-packages/transformers/modeling_utils.py:4833\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   4831\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m   4832\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m-> \u001b[39m\u001b[32m4833\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[32m   4852\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/data/conda_envs/sfllava/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/data/conda_envs/sfllava/lib/python3.12/site-packages/transformers/modeling_utils.py:824\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled():\n\u001b[32m    822\u001b[39m         param_device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     _load_parameter_into_model(model, param_name, \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    827\u001b[39m     hf_quantizer.create_quantized_param(\n\u001b[32m    828\u001b[39m         model, param, param_name, param_device, state_dict, unexpected_keys\n\u001b[32m    829\u001b[39m     )\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 14.31 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 31.41 GiB is allocated by PyTorch, and 6.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 2. 加载llava-next\n",
    "\n",
    "from transformers import LlavaProcessor, LlavaForConditionalGeneration\n",
    "from transformers import LlavaNextVideoProcessor, LlavaNextVideoForConditionalGeneration\n",
    "\n",
    "import torch\n",
    "model_name_or_path = \"/media/data/document/wpf_works/learn/learnllava_next_video/LLaVA-NeXT-Video-7B-hf\"\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    model_name_or_path, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# processor = LlavaNextVideoProcessor.from_pretrained(model_name_or_path)\n",
    "# inputs = llava_processor(text=prompt, images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb733d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "###test用\n",
    "\n",
    "image_path = \"/home/rootlab338/work/qwen2.5/data_hq/nature/churchbird.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "inputs= processor_qwen(text=[prompt_1],images=image,return_tensors=\"pt\",padding=True).to(device)\n",
    "generated_ids = model_qwen.generate(**inputs, max_new_tokens=512)\n",
    "trimmed_ids = [out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)]\n",
    "output_text = processor_qwen.batch_decode(trimmed_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d1f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/rootlab338/work/qwen2.5/data_hq/nature/churchbird.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "inputs= processor_qwen(text=[prompt_1],images=image,return_tensors=\"pt\",padding=True).to(device)\n",
    "generated_ids = model_qwen.generate(**inputs, max_new_tokens=512)\n",
    "trimmed_ids = [out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)]\n",
    "output_text = processor_qwen.batch_decode(trimmed_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "978c5a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:13<00:00, 22.27s/it]\n",
      "100%|██████████| 6/6 [02:03<00:00, 20.59s/it]\n",
      "100%|██████████| 7/7 [02:36<00:00, 22.39s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "img_dir_0 = Path(\"/home/rootlab338/work/qwen2.5/data_hq/artificial\")\n",
    "img_dir_1 = Path(\"/home/rootlab338/work/qwen2.5/data_hq/nature\")\n",
    "img_dir_2 = Path(\"/home/rootlab338/work/qwen2.5/data_hq/concat\")\n",
    "img_dirs = [img_dir_0,img_dir_1,img_dir_2 ]\n",
    "\n",
    "for img_dir in img_dirs:\n",
    "    results = []\n",
    "    prompts = [prompt_0,prompt_1]\n",
    "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".webp\"]\n",
    "    image_paths = sorted([p for p in img_dir.iterdir() if p.suffix.lower() in image_extensions])\n",
    "\n",
    "    for image_path in tqdm(image_paths):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        for prompt in prompts:\n",
    "            inputs= processor_qwen(\n",
    "                    text=[prompt],\n",
    "                    images=image,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True\n",
    "                ).to(device)\n",
    "            generated_ids = model_qwen.generate(**inputs, max_new_tokens=512)\n",
    "            trimmed_ids = [out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)]\n",
    "            output_text = processor_qwen.batch_decode(trimmed_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "            results.append({\n",
    "                    \"image\": image_path.name,\n",
    "                    f\"description_of_prompt\": output_text,\n",
    "                    \"prompt_token_count\": len(inputs.input_ids[0]),\n",
    "                    \"generated_token_count\": len(generated_ids[0]),\n",
    "                    \"answer_token_count\": len(trimmed_ids[0])\n",
    "            })\n",
    "            i += 1\n",
    "    output_path = img_dir / \"generated_descriptions.json\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6791c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 这张图片展示了一座城市的全景，画面中央是一座白色的圆顶建筑，顶部有一个黑色的星月标志，象征着伊斯兰教的信仰。这座建筑位于画面的中心偏右位置，周围环绕着许多低矮的砖瓦房屋，显示出一种典型的中东或南亚城市风貌。远处可以看到一些高大的通信塔，表明这是一个现代化的城市。\n",
      "\n",
      "天空呈现出灰蒙蒙的颜色，给人一种阴沉的感觉，可能是清晨或傍晚时分。空中有许多鸟儿在飞翔，其中一只飞得非常近，占据了画面的左上角，增加了画面的动感。这些鸟儿分散在不同的高度，有的靠近建筑物，有的则飞得更高，形成了一个动态的飞行图案。\n",
      "\n",
      "建筑物的屋顶上有一些晾晒的衣物，显示出居民的生活气息。整个画面给人一种宁静而略带忧郁的感觉，可能是由于阴沉的天气和城市中常见的建筑风格所造成的。\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c53b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
